{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZoUvCUdqq02w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c1cb08-3b0b-4a01-c976-5fe45e1541bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.12/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.12/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved ev_reddit.csv with 686 posts\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# üîë Your credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"wzx3QK7ygFrKPSQeXhzeWQ\",\n",
        "    client_secret=\"_aZlEAeqVWeVpZTW7Rjr40YCXjIAhA\",\n",
        "    user_agent =\"my user agent\"\n",
        ")\n",
        "\n",
        "# Search EV-related posts (example: r/electricvehicles, r/cars, r/technology)\n",
        "subreddits = [\"electricvehicles\", \"cars\", \"technology\"]\n",
        "posts = []\n",
        "\n",
        "for sub in subreddits:\n",
        "    for submission in reddit.subreddit(sub).search(\"EV OR Electric Vehicle OR Tesla\", limit=500):\n",
        "        posts.append({\n",
        "            \"id\": submission.id,\n",
        "            \"title\": submission.title,\n",
        "            \"selftext\": submission.selftext,\n",
        "            \"score\": submission.score,\n",
        "            \"created_utc\": submission.created_utc,\n",
        "            \"subreddit\": submission.subreddit.display_name,\n",
        "            \"url\": submission.url\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(posts)\n",
        "df.to_csv(\"ev_reddit.csv\", index=False)\n",
        "print(\"Saved ev_reddit.csv with\", len(df), \"posts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5agmVGL1q-Lh",
        "outputId": "01d7e296-d47d-4f0c-a9b5-acb24b8d466c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_ev_reddit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app_ev_reddit.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Reddit EV Text Analysis ‚Äî Research Upgrade\n",
        "- Live Reddit scraping (PRAW)\n",
        "- Sentiment: VADER or Transformer (RoBERTa)\n",
        "- Topics: LDA or BERTopic\n",
        "- Wordcloud, time-trends, downloads\n",
        "\"\"\"\n",
        "\n",
        "import os, numpy as np, pandas as pd, streamlit as st, matplotlib.pyplot as plt\n",
        "import praw\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# --- Optional, modern NLP ---\n",
        "_HAVE_TRANSFORMERS = False\n",
        "_HAVE_BERTOPIC = False\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "    _HAVE_TRANSFORMERS = True\n",
        "except Exception:\n",
        "    _HAVE_TRANSFORMERS = False\n",
        "\n",
        "try:\n",
        "    from bertopic import BERTopic\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    _HAVE_BERTOPIC = True\n",
        "except Exception:\n",
        "    _HAVE_BERTOPIC = False\n",
        "\n",
        "# Ensure VADER resource\n",
        "try:\n",
        "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
        "except LookupError:\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "\n",
        "st.set_page_config(page_title=\"Reddit EV Text Analysis (NLP+)\", layout=\"wide\")\n",
        "st.title(\"üîã Reddit Text Analysis: Electric Vehicles (EVs) ‚Äî Research Upgrade\")\n",
        "\n",
        "# =========================\n",
        "# Sidebar: API + Query\n",
        "# =========================\n",
        "st.sidebar.header(\"‚öôÔ∏è Reddit API\")\n",
        "with st.sidebar.expander(\"Credentials\", expanded=True):\n",
        "    client_id     = st.text_input(\"Client ID\")\n",
        "    client_secret = st.text_input(\"Client Secret\", type=\"password\")\n",
        "    user_agent    = st.text_input(\"User Agent\", value=\"ev_analysis_app\")\n",
        "\n",
        "st.sidebar.header(\"üîé Query\")\n",
        "subreddit_name = st.sidebar.text_input(\"Subreddit\", value=\"electricvehicles\")\n",
        "search_query   = st.sidebar.text_input(\"Search Query\", value=\"EV OR Tesla OR Electric Vehicle\")\n",
        "limit          = st.sidebar.slider(\"Number of posts\", 50, 1000, 300, 50)\n",
        "text_source    = st.sidebar.selectbox(\"Text to analyze\", [\"title\", \"selftext\", \"title + selftext\"])\n",
        "\n",
        "st.sidebar.header(\"üß† Models\")\n",
        "sent_choice = st.sidebar.selectbox(\n",
        "    \"Sentiment model\",\n",
        "    [\"Transformer (RoBERTa)\", \"VADER\"],\n",
        "    index=0 if _HAVE_TRANSFORMERS else 1,\n",
        "    help=\"Falls back to VADER if Transformers not installed\"\n",
        ")\n",
        "topic_choice = st.sidebar.selectbox(\n",
        "    \"Topic model\",\n",
        "    [\"BERTopic\", \"LDA\"],\n",
        "    index=0 if _HAVE_BERTOPIC else 1,\n",
        "    help=\"Falls back to LDA if BERTopic not installed\"\n",
        ")\n",
        "\n",
        "fetch_btn = st.sidebar.button(\"üîé Fetch Reddit Data\")\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "@st.cache_data(show_spinner=False)\n",
        "def fetch_reddit(client_id, client_secret, user_agent, subreddit, query, limit):\n",
        "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
        "    sub = reddit.subreddit(subreddit)\n",
        "    rows = []\n",
        "    for s in sub.search(query, limit=limit):\n",
        "        rows.append({\n",
        "            \"id\": s.id,\n",
        "            \"subreddit\": getattr(s.subreddit, \"display_name\", None),\n",
        "            \"title\": s.title,\n",
        "            \"selftext\": s.selftext,\n",
        "            \"score\": int(getattr(s, \"score\", 0) or 0),\n",
        "            \"num_comments\": int(getattr(s, \"num_comments\", 0) or 0),\n",
        "            \"created_utc\": float(getattr(s, \"created_utc\", np.nan)),\n",
        "            \"permalink\": f\"https://www.reddit.com{s.permalink}\" if getattr(s, \"permalink\", None) else None,\n",
        "            \"url\": getattr(s, \"url\", None),\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    if \"created_utc\" in df.columns:\n",
        "        df[\"created_dt\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", utc=True).dt.tz_convert(\"UTC\")\n",
        "        df[\"month\"] = df[\"created_dt\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "    return df\n",
        "\n",
        "def compose_text(df: pd.DataFrame, source: str) -> pd.Series:\n",
        "    if source == \"title\":\n",
        "        return df[\"title\"].fillna(\"\")\n",
        "    if source == \"selftext\":\n",
        "        return df[\"selftext\"].fillna(\"\")\n",
        "    return (df[\"title\"].fillna(\"\") + \" \" + df[\"selftext\"].fillna(\"\")).str.strip()\n",
        "\n",
        "def clean_text(s: pd.Series) -> pd.Series:\n",
        "    return (s.astype(str)\n",
        "            .str.replace(r\"http\\S+|www\\.\\S+\", \" \", regex=True)\n",
        "            .str.replace(r\"&amp;|&lt;|&gt;\", \" \", regex=True)\n",
        "            .str.replace(r\"[^\\w\\s\\-']\", \" \", regex=True)\n",
        "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "            .str.strip())\n",
        "\n",
        "# ---------- Sentiment ----------\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_transformer_sentiment():\n",
        "    # Lightweight, well-known model for general sentiment\n",
        "    model_id = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "    clf = pipeline(\"sentiment-analysis\", model=model_id, tokenizer=model_id, truncation=True)\n",
        "    return clf\n",
        "\n",
        "def sentiment_vader(texts: list[str]) -> tuple[np.ndarray, list[str]]:\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    scores = np.array([sia.polarity_scores(t)[\"compound\"] for t in texts], dtype=float)\n",
        "    labels = np.where(scores > 0.05, \"positive\", np.where(scores < -0.05, \"negative\", \"neutral\")).tolist()\n",
        "    return scores, labels\n",
        "\n",
        "def sentiment_transformer(texts: list[str]) -> tuple[np.ndarray, list[str]]:\n",
        "    clf = load_transformer_sentiment()\n",
        "    # batched inference to be safe\n",
        "    preds, scores = [], []\n",
        "    batch = 32\n",
        "    for i in range(0, len(texts), batch):\n",
        "        out = clf(texts[i:i+batch])\n",
        "        for r in out:\n",
        "            lab = r[\"label\"].lower()\n",
        "            # Map to [-1,1] like compound (rough heuristic)\n",
        "            if \"pos\" in lab:\n",
        "                preds.append(\"positive\"); scores.append(0.7 if \"neutral\" not in lab else 0.0)\n",
        "            elif \"neu\" in lab:\n",
        "                preds.append(\"neutral\"); scores.append(0.0)\n",
        "            else:\n",
        "                preds.append(\"negative\"); scores.append(-0.7)\n",
        "    return np.array(scores, dtype=float), preds\n",
        "\n",
        "# ---------- Topics ----------\n",
        "def topics_lda(texts: list[str], n_topics=6, max_features=4000):\n",
        "    vec = CountVectorizer(stop_words=\"english\", max_features=max_features)\n",
        "    X = vec.fit_transform(texts)\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method=\"batch\")\n",
        "    lda.fit(X)\n",
        "    vocab = vec.get_feature_names_out()\n",
        "    top_terms = []\n",
        "    for comp in lda.components_:\n",
        "        idx = comp.argsort()[-12:]\n",
        "        top_terms.append(\", \".join(vocab[idx]))\n",
        "    return top_terms\n",
        "\n",
        "def topics_bertopic(texts: list[str], nr_topics=None):\n",
        "    # sentence-transformers is auto-detected by BERTopic\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    topic_model = BERTopic(embedding_model=embedder, nr_topics=nr_topics, verbose=False)\n",
        "    topics, _ = topic_model.fit_transform(texts)\n",
        "    info = topic_model.get_topic_info()\n",
        "    # get top terms for first N non-outlier topics\n",
        "    terms = []\n",
        "    for topic_id in info[info.Topic >= 0].Topic.head(10):\n",
        "        words = [w for w, _ in topic_model.get_topic(topic_id)]\n",
        "        terms.append(\", \".join(words[:12]))\n",
        "    return terms\n",
        "\n",
        "# =========================\n",
        "# Fetch\n",
        "# =========================\n",
        "df = None\n",
        "if fetch_btn:\n",
        "    if not (client_id and client_secret and user_agent):\n",
        "        st.error(\"‚ö†Ô∏è Enter Reddit API credentials first.\")\n",
        "    else:\n",
        "        with st.spinner(\"Fetching posts from Reddit...\"):\n",
        "            df = fetch_reddit(client_id, client_secret, user_agent, subreddit_name, search_query, limit)\n",
        "        if df.empty:\n",
        "            st.warning(\"No posts returned. Try a different query/subreddit.\")\n",
        "        else:\n",
        "            st.success(f\"‚úÖ Retrieved {len(df)} posts from r/{subreddit_name}\")\n",
        "            st.dataframe(df.head(12), use_container_width=True)\n",
        "\n",
        "# =========================\n",
        "# Analysis\n",
        "# =========================\n",
        "if df is not None and not df.empty:\n",
        "    # Prepare text\n",
        "    df[\"text\"] = clean_text(compose_text(df, text_source))\n",
        "    df = df[df[\"text\"].str.len() > 0].copy()\n",
        "\n",
        "    # ----- Sentiment -----\n",
        "    st.markdown(\"## üìä Sentiment Analysis\")\n",
        "    use_transformer = (sent_choice.startswith(\"Transformer\") and _HAVE_TRANSFORMERS)\n",
        "    if sent_choice.startswith(\"Transformer\") and not _HAVE_TRANSFORMERS:\n",
        "        st.info(\"Transformers not installed ‚Äî falling back to VADER.\")\n",
        "    with st.spinner(\"Scoring sentiment...\"):\n",
        "        if use_transformer:\n",
        "            scores, labels = sentiment_transformer(df[\"text\"].tolist())\n",
        "        else:\n",
        "            scores, labels = sentiment_vader(df[\"text\"].tolist())\n",
        "    df[\"sentiment\"] = scores\n",
        "    df[\"sent_label\"] = labels\n",
        "\n",
        "    c1, c2 = st.columns([1,2])\n",
        "    with c1:\n",
        "        st.write(\"**Class balance**\")\n",
        "        st.bar_chart(df[\"sent_label\"].value_counts().reindex([\"negative\",\"neutral\",\"positive\"]).fillna(0))\n",
        "    with c2:\n",
        "        st.write(\"**Score distribution**\")\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.hist(df[\"sentiment\"].values, bins=30)\n",
        "        ax.set_xlabel(\"sentiment score\"); ax.set_ylabel(\"count\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    if df[\"month\"].notna().any():\n",
        "        st.write(\"**Average sentiment per month**\")\n",
        "        monthly = df.dropna(subset=[\"month\"]).groupby(\"month\")[\"sentiment\"].mean()\n",
        "        st.line_chart(monthly)\n",
        "\n",
        "    st.write(\"**Examples**\")\n",
        "    for _, r in df.sample(min(3, len(df)), random_state=42).iterrows():\n",
        "        st.markdown(f\"- **{r['sent_label'].capitalize()}** ‚Äî {r['title'] or '[no title]'}\")\n",
        "        if r.get(\"permalink\"):\n",
        "            st.caption(r[\"permalink\"])\n",
        "\n",
        "    # ----- Wordcloud -----\n",
        "    st.markdown(\"## ‚òÅÔ∏è Word Cloud\")\n",
        "    text_blob = \" \".join(df[\"text\"].tolist())[:3_000_000]\n",
        "    wc = WordCloud(width=1100, height=450, background_color=\"white\").generate(text_blob)\n",
        "    fig, ax = plt.subplots(figsize=(11,4))\n",
        "    ax.imshow(wc, interpolation=\"bilinear\"); ax.axis(\"off\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # ----- Topics -----\n",
        "    st.markdown(\"## üß© Topic Modeling\")\n",
        "    if topic_choice == \"BERTopic\" and not _HAVE_BERTOPIC:\n",
        "        st.info(\"BERTopic not installed ‚Äî falling back to LDA.\")\n",
        "\n",
        "    if topic_choice == \"BERTopic\" and _HAVE_BERTOPIC:\n",
        "        nr_topics = st.slider(\"Number of topics (approx, BERTopic may merge)\", 4, 20, 10, 1)\n",
        "        with st.spinner(\"Fitting BERTopic (embeddings + clustering)...\"):\n",
        "            try:\n",
        "                top_terms = topics_bertopic(df[\"text\"].tolist(), nr_topics)\n",
        "                st.write(\"**Top topics (first 10):**\")\n",
        "                for i, terms in enumerate(top_terms, 1):\n",
        "                    st.markdown(f\"- **Topic {i}:** {terms}\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"BERTopic failed: {e}. Falling back to LDA.\")\n",
        "                topic_choice = \"LDA\"  # force fallback\n",
        "    if topic_choice == \"LDA\" or not _HAVE_BERTOPIC:\n",
        "        n_topics     = st.slider(\"Number of topics (LDA)\", 3, 12, 6, 1)\n",
        "        max_features = st.slider(\"Max features (BoW)\", 1000, 10000, 4000, 500)\n",
        "        with st.spinner(\"Fitting LDA...\"):\n",
        "            try:\n",
        "                top_terms = topics_lda(df[\"text\"].tolist(), n_topics=n_topics, max_features=max_features)\n",
        "                st.write(\"**Top terms per topic**\")\n",
        "                for i, terms in enumerate(top_terms, start=1):\n",
        "                    st.markdown(f\"- **Topic {i}:** {terms}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"LDA failed: {e}\")\n",
        "\n",
        "    # ----- Downloads -----\n",
        "    st.markdown(\"## ‚¨áÔ∏è Save Results\")\n",
        "    enriched = df[[\n",
        "        \"id\",\"subreddit\",\"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\",\"created_dt\",\n",
        "        \"permalink\",\"url\",\"text\",\"sentiment\",\"sent_label\"\n",
        "    ]].copy()\n",
        "    st.download_button(\"Download enriched CSV\", enriched.to_csv(index=False), file_name=\"ev_reddit_enriched.csv\")\n",
        "    st.download_button(\"Download enriched JSON\", enriched.to_json(orient=\"records\"), file_name=\"ev_reddit_enriched.json\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Enter credentials, set your query, then click **Fetch Reddit Data** to begin.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit praw wordcloud scikit-learn nltk matplotlib pyngrok \\\n",
        "  transformers torch bertopic umap-learn hdbscan sentence-transformers\n",
        "\n",
        "!pkill -f streamlit || true\n",
        "!streamlit run app_ev_reddit.py --server.port 8501 &>/content/logs.txt &\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2tZ6mqHFZ9n2B4HsTOzAPVA3Jnw_6qB1RFncPLxV8kcYUxNcJ\")\n",
        "print(\"üåç Public URL:\", ngrok.connect(8501, \"http\").public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_5hJ9jpI3zq",
        "outputId": "a9a013d4-bdc2-4690-bc73-57f6cda76791"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/153.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h^C\n",
            "üåç Public URL: https://64f8e6eabe3b.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njUFrr2XuCMl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}